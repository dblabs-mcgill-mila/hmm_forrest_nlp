import nilearn
import nibabel
import os
import re
import glob
import pickle
import argparse
import numpy as np
import warnings

from scipy.stats import zscore
from nilearn.plotting import plot_roi
from scipy.io import savemat, loadmat
from sklearn.decomposition import PCA
from nilearn import datasets
from nilearn.input_data import NiftiMasker, NiftiLabelsMasker, NiftiMapsMasker

warnings.simplefilter(action='ignore', category=FutureWarning)
parser = argparse.ArgumentParser()
parser.add_argument("--subject", "-s", default="01")

tmp_dir = '' #TODO
ROOT_DIR = "" #TODO
args = parser.parse_args()
subject = args.subject

try:
    inverse_path = os.path.join(ROOT_DIR,
                                'aligned_data_fits', 'Schaefer_100')
    os.mkdir(inverse_path)

except OSError as error:
    print(error)


schaefer_timeseries_dict = {}
hippo_timeseries_pcs_dict = {}
amyg_timeseries_pcs_dict = {}
accumbens_timeseries_dict = {}
accumbens_pcs_timeseries_dict = {}
accumbens_pcs_explained_variance_dict = {}
accumbens_pcs_explained_variance_ratio_dict = {}

# set the numer of pcs for limbic regions in each hemisphere
n_ha = 3

#casting the Schaefer 100 maps to int
schaefer_100_maps = nilearn.image.load_img(
    '{0}/schaefer_100_maps.nii_sub-{1}_to_bold3Tp2.nii.gz'
    .format(ROOT_DIR, subject))
schaefer_100_maps_data = nilearn.image.get_data(schaefer_100_maps)
schaefer_100_maps_data = np.around(schaefer_100_maps_data).astype(int)
schaefer_100_maps = nibabel.nifti1.Nifti1Image(schaefer_100_maps_data,
                                               affine=schaefer_100_maps.affine)
schaefer_100_maps.to_filename(
    os.path.join(inverse_path,
                 'schaefer_100_maps_rounded_subject_{}'.format(subject)))

# hippocampus and amygdala maps
# the data was generated by freesurfer's "segmentHA_T1.sh" function, 
# the raw data is the structural T1 data offered by studyforrest
right_hippoAma = nilearn.image.load_img(
    f'{ROOT_DIR}/hippoAmygLabels/sub{subject}_rh.nii.gz'
)
left_hippoAma = nilearn.image.load_img(
    f'{ROOT_DIR}/hippoAmygLabels/sub{subject}_lh.nii.gz'
)
right_hippoAma_data = nilearn.image.get_data(right_hippoAma)
right_hippoAma_data = np.around(right_hippoAma_data).astype(int)
left_hippoAma_data = nilearn.image.get_data(left_hippoAma)
left_hippoAma_data = np.around(left_hippoAma_data).astype(int)
left_hippoAma_maps = nibabel.nifti1.Nifti1Image(left_hippoAma_data,
                                                affine=left_hippoAma.affine)
right_hippoAma_maps = nibabel.nifti1.Nifti1Image(right_hippoAma_data,
                                                 affine=right_hippoAma.affine)


#maskers
lh_masker = NiftiLabelsMasker(labels_img=left_hippoAma_maps,
                              standardize=True,
                              resampling_target='labels',
                              memory='nilearn_cache')
rh_masker = NiftiLabelsMasker(labels_img=right_hippoAma_maps,
                              standardize=True,
                              resampling_target='labels',
                              memory='nilearn_cache')
if yeo:
    schaefer_masker = NiftiLabelsMasker(labels_img=schaefer_100_maps,
                                    standardize=True,
                                    memory='nilearn_cache')
    
    
# load fMRI data. The data was downloaded from the studyforrest dataset
# https://github.com/psychoinformatics-de/studyforrest-data-aligned
runs = []
paths = glob.glob(
    '{1}/studyforrest-data-aligned/sub-{0}/in_bold3Tp2/sub-{0}_task-avmovie_run-*_bold.nii.gz'
    .format(subject, ROOT_DIR))

#correction of overlap between runs
runs.append(
    nilearn.image.index_img(nilearn.image.load_img(paths[0]), slice(-3)))
for run in range(1, len(paths) - 1):
    runs.append(
        nilearn.image.index_img(nilearn.image.load_img(paths[run]),
                                slice(5, -3)))
runs.append(
    nilearn.image.index_img(nilearn.image.load_img(paths[-1]), slice(5, None)))

#detrending and standardization for each run separately
for run in range(len(runs)):
    runs[run] = nilearn.image.clean_img(runs[run])

#concatenation and realignment on first subject affine if necessary
preprocessed_timeseries = nilearn.image.concat_imgs(runs, auto_resample=True)
runs_lengths = [448, 433, 430, 480, 454, 431, 534, 333]
np.save(f"{tmp_dir}/lh_masker.npy", lh_masker)
np.save(f"{tmp_dir}/rh_masker.npy", rh_masker)


schaefer_ts = []
lh = []
rh = []
ln = []
rn = []
cum_runs_length = 0
run = 1
print('Start!')
#signal extraction for each run separately to avoid overloading memory
for run_length in runs_lengths:

    imgs = nilearn.image.index_img(
        preprocessed_timeseries,
        slice(cum_runs_length, cum_runs_length + run_length))
    if yeo:
        schaefer_timeseries = schaefer_masker.fit_transform(imgs)
        scheafer_inverse = schaefer_masker.inverse_transform(
            schaefer_timeseries[:30, :])
        scheafer_inverse.to_filename(
            os.path.join(inverse_path,
                         'schaefer_inverse_subject_{}'.format(subject)))
        schaefer_ts.append(schaefer_timeseries)

    lh_trans = lh_masker.fit_transform(imgs)
    rh_trans = rh_masker.fit_transform(imgs)
    lh.append(lh_trans)
    rh.append(rh_trans)

    cum_runs_length += run_length
    run += 1

#standardization across time
schaefer_timeseries_z_scored = zscore(np.vstack(schaefer_ts))
schaefer_timeseries_dict[subject] = schaefer_timeseries_z_scored

lh_timeseries = np.vstack(lh)
rh_timeseries = np.vstack(rh)
lh_hippo = zscore(lh_timeseries[:, :19])
lh_amyg = zscore(lh_timeseries[:, 19:])
rh_hippo = zscore(rh_timeseries[:, :19])
rh_amyg = zscore(rh_timeseries[:, 19:])
print(lh_hippo.shape, lh_amyg.shape, rh_hippo.shape, rh_amyg.shape)

pca = PCA(n_components=n_ha, random_state=0)
pca.fit(np.vstack([lh_hippo, rh_hippo]))
np.save(f"{tmp_dir}/hippo_pca_{subject}.npy", pca)
hippo_timeseries_pcs = zscore(
    np.hstack([pca.transform(lh_hippo),
               pca.transform(rh_hippo)]))
hippo_components = pca.components_
pca.fit(np.vstack([lh_amyg, rh_amyg]))
np.save(f"{tmp_dir}/amyg_pca_{subject}.npy", pca)
amyg_timeseries_pcs = zscore(
    np.hstack([pca.transform(lh_amyg),
               pca.transform(rh_amyg)]))
amyg_components = pca.components_

hippo_timeseries_pcs_dict[subject] = hippo_timeseries_pcs
amyg_timeseries_pcs_dict[subject] = amyg_timeseries_pcs

print('subject {}: done'.format(subject))

# uncomment the following lines to save the preprocessed data
# with open(f'{tmp_dir}/hippoAmyg_timeseries_{subject}.pickle',
#           'wb') as handle:
#       pickle.dump([lh_hippo, lh_amyg, rh_hippo, rh_amyg], handle)
# with open(f'{tmp_dir}/hippo_timeseries_{n_ha}pcs_{subject}.pickle',
#           'wb') as handle:
#     pickle.dump(hippo_timeseries_pcs_dict[subject], handle)
# with open(f'{tmp_dir}/amyg_timeseries_{n_ha}pcs_{subject}.pickle',
#           'wb') as handle:
#     pickle.dump(amyg_timeseries_pcs_dict[subject], handle)
# with open(f'{tmp_dir}/schaefer_timeseries_{subject}.pickle', 'wb') as handle:
#     pickle.dump(schaefer_timeseries_dict[subject], handle)
# with open(f'{tmp_dir}/accumbens_timeseries_{subject}.pickle', 'wb') as handle:
#     pickle.dump(accumbens_timeseries_dict[subject], handle)
# with open(f'{tmp_dir}/accumbens_{n_ha}pcs_timeseries_{subject}.pickle',
#           'wb') as handle:
#     pickle.dump(accumbens_pcs_timeseries_dict[subject], handle)
# with open(f'{tmp_dir}/accumbens_{n_ha}pcs_explained_variance_{subject}.pickle',
#           'wb') as handle:
#     pickle.dump(accumbens_pcs_explained_variance_dict[subject], handle)
# with open(
#         f'{tmp_dir}/accumbens_{n_ha}pcs_explained_variance_ratio_{subject}.pickle',
#         'wb') as handle:
#     pickle.dump(accumbens_pcs_explained_variance_ratio_dict[subject], handle)
